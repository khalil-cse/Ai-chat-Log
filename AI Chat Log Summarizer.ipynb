{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pip install nltk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# You must need to download those toolkits \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:00:03.410902Z",
     "iopub.status.busy": "2025-05-29T18:00:03.409688Z",
     "iopub.status.idle": "2025-05-29T18:00:03.417452Z",
     "shell.execute_reply": "2025-05-29T18:00:03.416215Z",
     "shell.execute_reply.started": "2025-05-29T18:00:03.410864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:00:05.907742Z",
     "iopub.status.busy": "2025-05-29T18:00:05.907439Z",
     "iopub.status.idle": "2025-05-29T18:00:05.914697Z",
     "shell.execute_reply": "2025-05-29T18:00:05.913526Z",
     "shell.execute_reply.started": "2025-05-29T18:00:05.907719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_chat_log(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    user_msgs = []\n",
    "    ai_msgs = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"User:\"):\n",
    "            user_msgs.append(line[len(\"User:\"):].strip())\n",
    "        elif line.startswith(\"AI:\"):\n",
    "            ai_msgs.append(line[len(\"AI:\"):].strip())\n",
    "\n",
    "    return user_msgs, ai_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:00:08.840011Z",
     "iopub.status.busy": "2025-05-29T18:00:08.839649Z",
     "iopub.status.idle": "2025-05-29T18:00:08.846005Z",
     "shell.execute_reply": "2025-05-29T18:00:08.844913Z",
     "shell.execute_reply.started": "2025-05-29T18:00:08.839984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_and_lemmatize(text_list):\n",
    "    cleaned = []\n",
    "    for text in text_list:\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in STOP_WORDS]\n",
    "        cleaned.append(' '.join(tokens))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:00:12.177225Z",
     "iopub.status.busy": "2025-05-29T18:00:12.176942Z",
     "iopub.status.idle": "2025-05-29T18:00:12.183349Z",
     "shell.execute_reply": "2025-05-29T18:00:12.182089Z",
     "shell.execute_reply.started": "2025-05-29T18:00:12.177205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_keywords_tfidf(text_list, top_n=5):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_list)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    scores = tfidf_matrix.sum(axis=0).A1\n",
    "    word_scores = dict(zip(feature_names, scores))\n",
    "\n",
    "    sort_keys = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, score in sort_keys[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:00:15.464073Z",
     "iopub.status.busy": "2025-05-29T18:00:15.463719Z",
     "iopub.status.idle": "2025-05-29T18:00:15.470003Z",
     "shell.execute_reply": "2025-05-29T18:00:15.468790Z",
     "shell.execute_reply.started": "2025-05-29T18:00:15.464047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_topic_summary(keywords):\n",
    "    if not keywords:\n",
    "        return\n",
    "\n",
    "    if len(keywords) == 1:\n",
    "        return f\"The conversation was mainly about {keywords[0]}.\"\n",
    "    elif len(keywords) == 2:\n",
    "        return f\"The conversation was mainly about {keywords[0]} and {keywords[1]}.\"\n",
    "    else:\n",
    "        main_part = \", \".join(keywords[:-1])\n",
    "        return f\"The conversation was mainly about {main_part}, and {keywords[-1]}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:00:36.612813Z",
     "iopub.status.busy": "2025-05-29T18:00:36.612487Z",
     "iopub.status.idle": "2025-05-29T18:00:36.619545Z",
     "shell.execute_reply": "2025-05-29T18:00:36.618471Z",
     "shell.execute_reply.started": "2025-05-29T18:00:36.612789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def summarize_chat(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    user_msgs, ai_msgs = parse_chat_log(file_path)\n",
    "    all_msgs = user_msgs + ai_msgs\n",
    "    total_exchanges = len(all_msgs)\n",
    "\n",
    "    cleaned_msgs = clean_and_lemmatize(all_msgs)\n",
    "    top_keywords = extract_keywords_tfidf(cleaned_msgs)\n",
    "    \n",
    "    topic_summary = generate_topic_summary(top_keywords)\n",
    "\n",
    "    print(f\"Total exchanges        : {total_exchanges}\")\n",
    "    print(f\"User messages          : {len(user_msgs)}\")\n",
    "    print(f\"AI messages            : {len(ai_msgs)}\")\n",
    "    print(f\"Nature of conversation : {topic_summary}\")\n",
    "    print(f\"Most common keywords   : {', '.join(top_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:00:39.791429Z",
     "iopub.status.busy": "2025-05-29T18:00:39.790411Z",
     "iopub.status.idle": "2025-05-29T18:00:39.804517Z",
     "shell.execute_reply": "2025-05-29T18:00:39.803572Z",
     "shell.execute_reply.started": "2025-05-29T18:00:39.791390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total exchanges        : 34\n",
      "User messages          : 17\n",
      "AI messages            : 17\n",
      "Nature of conversation : The conversation was mainly about learning, machine, deep, unsupervised, and supervised.\n",
      "Most common keywords   : learning, machine, deep, unsupervised, supervised\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chat_log_path = \"/kaggle/input/simple-chatlog/Chat_log.txt\"\n",
    "    summarize_chat(chat_log_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7530630,
     "sourceId": 11974914,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
